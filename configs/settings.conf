#########################################################
# Default training settings, overwritten if specified
#########################################################
[Default]
dataset = MNIST
model_type = FC

dims = 64

scheduler = OneCycle
optimizer = Adam
n_steps = 1000
lr = 0.0003
max_lr = 0.01
batch_size = 1000
momentum = 0.9
dropout_prob = 0.0
weight_decay = 0.01

lr_ref = 0.002
n_steps_ref = 2000
conv_lr_ref = 2.0
conv_n_steps_ref = 4000

#########################################################
# FC models, direct representation
#########################################################
[FC]
dataset = MNIST
model_type = FC
layer_size = 784,400,10

[FC_2layer]
dataset = MNIST
model_type = FC
layer_size = 784,400,400,10

[FC_4layer]
dataset = MNIST
model_type = FC
layer_size = 784,400,400,400,400,10

[FC_8layer]
dataset = MNIST
model_type = FC
layer_size = 784,400,400,400,400,400,400,400,400,10

max_lr = 0.003

# for half MNIST problem
[FC_half]
dataset = MNIST_0_to_4
model_type = FC
layer_size = 784,400,5

max_lr = 0.003

[FC_half_2layer]
dataset = MNIST_0_to_4
model_type = FC
layer_size = 784,400,400,5

max_lr = 0.003

[FC_half_4layer]
dataset = MNIST_0_to_4
model_type = FC
layer_size = 784,400,400,400,400,5

max_lr = 0.003

[FC_deep]
dataset = MNIST
model_type = FC
layer_size = 784,400,200,100,30,10

[FC_CIFAR]
dataset = CIFAR10
model_type = FC
layer_size = 3072,400,400,400,200,10
#########################################################
# FC models, neuron embedding representation
#########################################################

[Emb]
dataset = MNIST
model_type = Emb
layer_size = 784,400,10

[Emb_2layer]
dataset = MNIST
model_type = Emb
layer_size = 784,400,400,10

[Emb_4layer]
dataset = MNIST
model_type = Emb
layer_size = 784,400,400,400,400,10

[Emb_8layer]
dataset = MNIST
model_type = Emb
layer_size = 784,400,400,400,400,400,400,400,400,10
max_lr = 0.003

# for half MNIST problem
[Emb_half]
dataset = MNIST_0_to_4
model_type = Emb
layer_size = 784,400,5

[Emb_half_2layer]
dataset = MNIST_0_to_4
model_type = Emb
layer_size = 784,400,400,5

[Emb_half_4layer]
dataset = MNIST_0_to_4
model_type = Emb
layer_size = 784,400,400,400,400,5

[Emb_deep]
dataset = MNIST
model_type = Emb
layer_size = 784,400,200,100,30,10

#########################################################
# Convolutional models, direct representation
#########################################################

# [Conv]
# dataset = MNIST
# model_type = Conv
# conv_layer_size = 1,16,40
# lin_layer_size = 1000,100,10
# kernel_size = 3

[ConvCIFAR]
dataset = CIFAR10
model_type = Conv
conv_layer_size = 3,64,64,64,64
lin_layer_size = 256,100,10
kernel_size = 3

# LeNet, needs MNIST32 for proper input shape
[LeNet]
dataset = MNIST32
model_type = Conv
conv_layer_size = 1,6,16
lin_layer_size = 400,120,84,10
kernel_size = 5

#########################################################
# Convolutional models, neuron embedding representation
#########################################################

[Conv]
dataset = MNIST
model_type = SeparableEmbConv
conv_type = separable
conv_layer_size = 1,20,40,80
lin_layer_size = 720,100,10
kernel_size = 3
dims = 64

[EmbConv]
dataset = MNIST
model_type = SeparableEmbConv
conv_type = embedded
conv_layer_size = 1,20,40,80
lin_layer_size = 720,100,10
kernel_size = 3
dims = 64

[SeparableEmbConvCIFAR]
dataset = CIFAR10
model_type = SeparableEmbConv
conv_type = separable
conv_layer_size = 3,64,64,64,64
lin_layer_size = 256,100,10
kernel_size = 3
dims = 48

[ResNet9]
dataset = CIFAR10
model_type = ResNet9
conv_type = standard
conv_layer_size = 3,64,128,128,128,256,256,256,256
lin_layer_size = 256,10
kernel_size = 3
dims = 48

optimizer = SGD
n_steps = 2000
max_lr = 0.0002
batch_size = 512
weight_decay = 0.512

[ResNet9Wide_100]
dataset = CIFAR100
model_type = ResNet9
conv_type = embedded
conv_layer_size = 3,64,256,256,256,512,1024,1024,1024
lin_layer_size = 1024,100
kernel_size = 3
dims = 48

optimizer = SGD
n_steps = 2000
max_lr = 0.0001
batch_size = 512
weight_decay = 0.256

[ResNet9Wide]
dataset = CIFAR10
model_type = ResNet9
conv_type = embedded
conv_layer_size = 3,64,256,256,256,512,1024,1024,1024
lin_layer_size = 1024,10
kernel_size = 3
dims = 48

optimizer = SGD
n_steps = 2000
max_lr = 0.0001
batch_size = 512
weight_decay = 0.256

[ResNet9SGD]
dataset = CIFAR10
model_type = ResNet9
conv_type = standard
conv_layer_size = 3,64,128,128,128,256,256,256,256
lin_layer_size = 256,10
kernel_size = 3
dims = 48

scheduler = OneCycle
optimizer = SGD
n_steps = 1500
lr = 0.0003
# max_lr is 0.4/batch_size
max_lr = 0.00078125
momentum = 0.9
batch_size = 512


# LeNet, needs MNIST32 for proper input shape
[Emb_LeNet]
dataset = MNIST32
model_type = EmbConv
conv_layer_size = 1,6,16
lin_layer_size = 400,120,84,10
kernel_size = 5
dims = 48

#########################################################
# Crossover test models
#########################################################

[Crossover FC]
dataset = MNIST
model_type = FC
layer_size = 784,400,10

[Crossover Emb]
dataset = MNIST
model_type = Emb
layer_size = 784,400,10
dims = 64

